{"cells":[{"cell_type":"code","source":["%fs ls  /FileStore/tables/movie_metadata1.csv"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%sql DROP TABLE IF EXISTS movie_metadata"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%sql CREATE TABLE movie_metadata (\n  color STRING,\n  director_name STRING,\n  num_critic_for_reviews DOUBLE,\n  duration DOUBLE,\n  director_facebook_likes DOUBLE,\n  actor_3_facebook_likes DOUBLE,\n  actor_2_name DOUBLE,\n  actor_1_facebook_likes DOUBLE,\n  gross DOUBLE,\n  genres STRING,\n  actor_1_name STRING,\n  movie_title STRING,\n  num_voted_users DOUBLE,\n  cast_total_facebook_likes DOUBLE,\n  actor_3_name STRING,\n  facenumber_in_poster DOUBLE,\n  plot_keywords STRING,\n  movie_imdb_link STRING,\n  num_user_for_reviews DOUBLE,\n  language STRING,\n  country STRING,\n  content_rating STRING,\n  budget DOUBLE,\n  title_year DOUBLE,\n  actor_2_facebook_likes DOUBLE,\n  imdb_score DOUBLE,\n  aspect_ratio DOUBLE,\n  movie_facebook_likes DOUBLE\n  )\n  \nUSING com.databricks.spark.csv\nOPTIONS (path \"/FileStore/tables/movie_metadata.csv\", header \"true\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["dataset = spark.table(\"movie_metadata\")\ncols = dataset.columns\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df=dataset.drop( 'color' ,\n  'director_name' ,\n  'num_critic_for_reviews' ,\n  \n  \n  \n  'actor_2_name' ,\n  \n  'gross' ,\n  'genres' ,\n  'actor_1_name' ,\n  'movie_title' ,\n  'num_voted_users' ,\n  'cast_total_facebook_likes' ,\n  'actor_3_name' ,\n  \n  'plot_keywords' ,\n  'movie_imdb_link' ,\n  'num_user_for_reviews' ,\n  'language' ,\n  'country' ,\n  'content_rating' ,\n  \n  'title_year' ,\n  \n   \n  'aspect_ratio' ,\n  'movie_facebook_likes' )\ndf = df.na.drop()\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql.functions import col  # for indicating a column using a string in the line below\ndf = df.select([col(c).cast(\"double\").alias(c) for c in df.columns])\ndf.printSchema()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nfeaturesCols = df.columns\nfeaturesCols.remove('imdb_score')\nassembler = VectorAssembler(\n    inputCols=featuresCols,\n    outputCol=\"features\")\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\n# Create a Pipeline.\npipeline = Pipeline(stages=[assembler])\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\npipelineModel = pipeline.fit(df)\ndf = pipelineModel.transform(df)\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["selectedcols = [\"imdb_score\", \"features\"]\ndf = df.select(selectedcols)\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["trainingData, testData = df.randomSplit([0.8, 0.2])\nprint trainingData.count()\nprint testData.count()\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\n# Create an initial RandomForest model.\nrf = RandomForestClassifier(labelCol=\"imdb_score\", featuresCol=\"features\")\n\n# Train model with Training Data\nrfModel = rf.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Make predictions on test data using the Transformer.transform() method.\npredictions = rfModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["predictions.printSchema()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# View model's predictions and probabilities of each prediction class\nselected = predictions.select(\"imdb_score\", \"prediction\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=rf.getLabelCol(), predictionCol=rf.getPredictionCol())\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Create ParamGrid for Cross Validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [2, 4, 6])\n             .addGrid(rf.maxBins, [20, 60])\n             .addGrid(rf.numTrees, [5, 20])\n             .build())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=paramGrid, numFolds=5)\n\ncvModel = cv.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Use test set here so we can measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# View Best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"imdb_score\", \"prediction\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["bestModel = cvModel.bestModel"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# Generate predictions for entire dataset\nfinalPredictions = bestModel.transform(df)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["\n"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"Project","notebookId":1968510312413274},"nbformat":4,"nbformat_minor":0}
